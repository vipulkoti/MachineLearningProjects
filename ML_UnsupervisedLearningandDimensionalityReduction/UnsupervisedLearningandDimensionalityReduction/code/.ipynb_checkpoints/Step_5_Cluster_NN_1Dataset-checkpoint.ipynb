{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43d32148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.random_projection import GaussianRandomProjection as GRP\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from scipy.stats import kurtosis\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import plot_confusion_matrix,f1_score,confusion_matrix,classification_report,accuracy_score,roc_auc_score,recall_score,mean_squared_error\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8728b8e9",
   "metadata": {},
   "source": [
    "<h1>Import Clean and Cluster</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8cc78f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.indexes.base.Index'>\n"
     ]
    }
   ],
   "source": [
    "#Wine Data\n",
    "data = pd.read_csv('../data/wineQualityDataset/WineQT.csv', sep=',')\n",
    "#Drop id\n",
    "data = data.drop(\"Id\", axis=1)\n",
    "#drop duplicate\n",
    "data = data.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "data = data.drop(data[data.quality == 3].index).reset_index(drop=True)\n",
    "data = data.drop(data[data.quality == 4].index).reset_index(drop=True)\n",
    "#data = data.drop(data[data.quality == 8].index).reset_index(drop=True)\n",
    "data.loc[data[\"quality\"] == 8, \"quality\"] = 7\n",
    "\n",
    "#normalize the data\n",
    "# Select numerical variables\n",
    "data_X = data.drop(\"quality\", axis=1)\n",
    "# data_X = data_X.drop(\"Id\", axis=1)\n",
    "numeric = data_X.select_dtypes(exclude=object).columns\n",
    "#numeric.remove('quality')\n",
    "print(type(numeric))\n",
    "for col in numeric:\n",
    "    data_X[col] = StandardScaler().fit_transform(data_X[[col]])\n",
    "\n",
    "data = data_X.join(data['quality'])\n",
    "\n",
    "# #update y values\n",
    "# data.loc[data[\"quality\"] == 5, \"quality\"] = 0\n",
    "# data.loc[data[\"quality\"] == 6, \"quality\"] = 1\n",
    "# data.loc[data[\"quality\"] == 7, \"quality\"] = 2\n",
    "\n",
    "#below vars are used.\n",
    "data_W = data\n",
    "wineX=data_W.drop(\"quality\", 1).copy().values\n",
    "winey= data_W[[\"quality\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20d870b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function based on:\n",
    "# https://chrisalbon.com/machine_learning/model_evaluation/plot_the_learning_curve/\n",
    "def plot_learning_curve(clf, X, y, title='Learning Curve'):\n",
    "    train_sizes, train_scores, test_scores, fit_times, score_times = learning_curve(\n",
    "                                                        clf, \n",
    "                                                        X, \n",
    "                                                        y,\n",
    "                                                        # Number of folds in cross-validation\n",
    "                                                        cv=5,\n",
    "                                                        # Evaluation metric\n",
    "                                                        scoring='recall',\n",
    "                                                        # Use all computer cores\n",
    "                                                        #n_jobs=-1, \n",
    "                                                        # 50 different sizes of the training set\n",
    "                                                        #train_sizes=np.linspace(0.01, 1.0, 10),\n",
    "                                                        train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "                                                        return_times = True,\n",
    "                                                        #random_state = 0\n",
    "                                                        )\n",
    "\n",
    "#     _, train_scores, test_scores = learning_curve(clf_nn_l, X_train, y_train, train_sizes=train_sizes, scoring='recall', cv=5)\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------\n",
    "    # Plot Learning Curve\n",
    "    # ------------------------------------------------------------------------------------------\n",
    "    # Create means and standard deviations of training set scores\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "\n",
    "    # Create means and standard deviations of test set scores\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    # Draw lines\n",
    "    plt.figure(1)\n",
    "    plt.plot(train_sizes, train_mean, '--', label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_mean, 'o-', label=\"Cross-validation score\")\n",
    "    # Draw bands\n",
    "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.2)\n",
    "    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.2)\n",
    "\n",
    "    # Create learning curve plot\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Training Set Size\"), plt.ylabel(\"Recall Score\"), plt.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    # ------------------------------------------------------------------------------------------\n",
    "    # Plot Scalability Curve\n",
    "    # ------------------------------------------------------------------------------------------\n",
    "    # Create mean and std of training times\n",
    "    train_times_mean = np.mean(fit_times, axis=1)\n",
    "    train_times_std  = np.std(fit_times, axis=1)\n",
    "    \n",
    "    # plot lines\n",
    "    plt.figure(2)\n",
    "    plt.plot(train_sizes, train_times_mean, 'o-')\n",
    "    plt.fill_between(train_sizes, train_times_mean - train_times_std, train_times_mean + train_times_std, color='#DDDDDD')\n",
    "    plt.title('Training Scalability')\n",
    "    plt.xlabel('Training Set Size')\n",
    "    plt.ylabel('Fit Times (s)')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "def plot_learning_curve2(clf, X_train, y_train, title='Learning Curve'):  \n",
    "    param_range=np.linspace(0.1, 1.0, 5)\n",
    "    train_sizes, train_scores, test_scores, fit_times, score_times= learning_curve(clf_nn_best, X_train, y_train, train_sizes=param_range, return_times = True, scoring='f1_micro', cv=5)\n",
    "\n",
    "    \n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.plot(train_sizes, np.mean(train_scores, axis=1), 'o-', label='Training score')\n",
    "    plt.plot(train_sizes, np.mean(test_scores, axis=1), 'o-', label='Cross-validation score')\n",
    "\n",
    "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"gainsboro\")\n",
    "    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"gainsboro\")\n",
    "\n",
    "    plt.title('Learning curve for neural network')\n",
    "    plt.xlabel('Fraction of training examples')\n",
    "    plt.ylabel(\"f1_micro score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid()\n",
    "#     plt.savefig('../images/wq/wq_nn_lc.png')\n",
    "    plt.show()   \n",
    "    \n",
    "        # ------------------------------------------------------------------------------------------\n",
    "    # Plot Scalability Curve\n",
    "    # ------------------------------------------------------------------------------------------\n",
    "    # Create mean and std of training times\n",
    "    train_times_mean = np.mean(fit_times, axis=1)\n",
    "    train_times_std  = np.std(fit_times, axis=1)\n",
    "    \n",
    "    # plot lines\n",
    "    plt.figure(2)\n",
    "    plt.plot(param_range, train_times_mean, 'o-')\n",
    "    plt.fill_between(param_range, train_times_mean - train_times_std, train_times_mean + train_times_std, color='gainsboro')\n",
    "    plt.title('Training Scalability')\n",
    "    plt.xlabel('Training Set Size')\n",
    "    plt.ylabel('Fit Times (s)')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "# Function based on:\n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "def plot_confusion(clf, X_test, y_test):\n",
    "    \n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    titles_options = [(\"Confusion matrix, without normalization\", None),\n",
    "                      (\"Normalized confusion matrix\", 'true')]\n",
    "    \n",
    "    for title, normalize in titles_options:\n",
    "        disp = plot_confusion_matrix(clf, X_test, y_test,\n",
    "                                     cmap=plt.cm.Blues,\n",
    "                                     normalize=normalize)\n",
    "        disp.ax_.set_title(title)\n",
    "\n",
    "        #print(title)\n",
    "        #print(disp.confusion_matrix)\n",
    "        plt.show()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return 0\n",
    "\n",
    "def plot_loss_curve(classifier_neural_network, X_train, y_train, X_test, y_test, title='Loss Curve NN'):\n",
    "\n",
    "#     classifier_neural_network = MLPClassifier(random_state=33, max_iter=1, learning_rate_init=best_clf_nn.best_params_['learning_rate_init'], hidden_layer_sizes=best_clf_nn.best_params_['hidden_layer_sizes'], warm_start=True)\n",
    "    epochs = 300\n",
    "    loss_training = np.zeros(epochs)\n",
    "    score_training = np.zeros(epochs)\n",
    "    score_validation = np.zeros(epochs)\n",
    "\n",
    "    X_train_train, X_train_val, y_train_train, y_train_val = train_test_split(X_train, y_train, test_size=0.3, random_state=33)\n",
    "    for epoch in range(epochs):\n",
    "        classifier_neural_network.fit(X_train_train, y_train_train)\n",
    "    \n",
    "        score_training[epoch] = f1_score(y_train_train, classifier_neural_network.predict(X_train_train), average='micro')\n",
    "        score_validation[epoch] = f1_score(y_train_val, classifier_neural_network.predict(X_train_val), average='micro')\n",
    "    \n",
    "        loss_training[epoch] = classifier_neural_network.loss_\n",
    "    \n",
    "    score_test = f1_score(y_test, classifier_neural_network.predict(X_test), average='micro')\n",
    "    print(\"f1_micro Score:\", score_test)\n",
    "    \n",
    "    plt.figure(3)\n",
    "    plt.plot(np.arange(epochs)+1, loss_training, label='Train Loss')\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Number of Epochs\")\n",
    "    plt.ylabel(\"Training Loss\")\n",
    "    plt.grid()\n",
    "#     plt.savefig('../images/wq/wq_nn_loss_curve.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a75740bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "\n",
    "# cluster data and predict labels\n",
    "kmeans = KMeans(n_clusters=7,init='k-means++', random_state=0)\n",
    "kmeans.fit(wineX)\n",
    "kmeans_labels = kmeans.predict(wineX)\n",
    "\n",
    "# add in cluster labels as additional feature\n",
    "kmeans_wineX = np.c_[wineX, kmeans_labels]\n",
    "\n",
    "X = kmeans_wineX\n",
    "y = winey.values.ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530759c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Neural net\n",
    "\n",
    "param_grid = {'learning_rate_init': np.logspace(-3, -1, 3), 'hidden_layer_sizes': np.arange(2,25,2)}\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "clf= GridSearchCV(MLPClassifier(random_state=33, max_iter=2000), param_grid=param_grid, scoring='f1_micro', cv=5)\n",
    "            \n",
    "t0 = time()\n",
    "clf.fit(X_train, y_train)\n",
    "nn_time = time() - t0\n",
    "\n",
    "print(\"Neural Net model fit in %.3f s\" % nn_time)\n",
    "print('Best Score: %.3f using %r' % (clf.best_score_, clf.best_params_))\n",
    "\n",
    "# Create NN with best parameters from grid search\n",
    "train_sizes = np.linspace(0.1, 1.0, 5)\n",
    "clf_nn_best = MLPClassifier(random_state=33, max_iter=2000, hidden_layer_sizes=clf.best_params_['hidden_layer_sizes'], learning_rate_init=clf.best_params_['learning_rate_init'])\n",
    "plot_learning_curve2(clf_nn_best, X_train, y_train, title='Learning Curve')\n",
    "# plot_loss_curve(clf_nn_best, X_train, y_train, X_test, y_test, title='K_Means, Loss Curve NN')\n",
    "\n",
    "\n",
    "    # Predict results using the test set\n",
    "clf_nn_best.fit(X_train, y_train)\n",
    "nn_pred = clf_nn_best.predict(X_test)\n",
    "    \n",
    "    \n",
    "    # let's see how our model performed\n",
    "print(classification_report(y_test, nn_pred))\n",
    "    \n",
    "plot_confusion(clf_nn_best, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451524ce",
   "metadata": {},
   "source": [
    "<h3>EM</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dab680f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create dataset\n",
    "\n",
    "# cluster data and predict labels\n",
    "em = GaussianMixture(n_components=6, random_state=0)\n",
    "em.fit(wineX)\n",
    "em_labels = em.predict(wineX)\n",
    "\n",
    "# add in cluster labels as additional feature\n",
    "em_wineX = np.c_[wineX, em_labels]\n",
    "\n",
    "X = em_wineX\n",
    "y = winey.values.ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73463378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Neural net\n",
    "\n",
    "param_grid = {'learning_rate_init': np.logspace(-3, -1, 3), 'hidden_layer_sizes': np.arange(2,25,2)}\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "clf= GridSearchCV(MLPClassifier(random_state=33, max_iter=2000), param_grid=param_grid, scoring='f1_micro', cv=5)\n",
    "            \n",
    "t0 = time()\n",
    "clf.fit(X_train, y_train)\n",
    "nn_time = time() - t0\n",
    "\n",
    "print(\"Neural Net model fit in %.3f s\" % nn_time)\n",
    "print('Best Score: %.3f using %r' % (clf.best_score_, clf.best_params_))\n",
    "\n",
    "# Create NN with best parameters from grid search\n",
    "train_sizes = np.linspace(0.1, 1.0, 5)\n",
    "clf_nn_best = MLPClassifier(random_state=33, max_iter=2000, hidden_layer_sizes=clf.best_params_['hidden_layer_sizes'], learning_rate_init=clf.best_params_['learning_rate_init'])\n",
    "plot_learning_curve2(clf_nn_best, X_train, y_train, title='Learning Curve')\n",
    "# plot_loss_curve(clf_nn_best, X_train, y_train, X_test, y_test, title= 'EM, Loss Curve NN')\n",
    "\n",
    "    # Predict results using the test set\n",
    "clf_nn_best.fit(X_train, y_train)\n",
    "nn_pred = clf_nn_best.predict(X_test)\n",
    "    \n",
    "    \n",
    "    # let's see how our model performed\n",
    "print(classification_report(y_test, nn_pred))\n",
    "    \n",
    "plot_confusion(clf_nn_best, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4e1598",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
