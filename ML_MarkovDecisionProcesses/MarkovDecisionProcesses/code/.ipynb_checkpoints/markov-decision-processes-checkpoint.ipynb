{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install mdptoolbox-hiive\n",
    "! pip install gym\n",
    "! pip install pymdptoolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import hiive.mdptoolbox \n",
    "import hiive.mdptoolbox.mdp\n",
    "import hiive.mdptoolbox.example\n",
    "import mdptoolbox, mdptoolbox.example\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# P, R = hiive.mdptoolbox.example.forest(S=2000, p=.01)\n",
    "# pim_test = hiive.mdptoolbox.mdp.ValueIteration(P, R, 0.999999, epsilon=0.01, max_iter=1000)\n",
    "# pim_test.run()\n",
    "# pim_test.run_stats\n",
    "# forest_pi_mdp = mdptoolbox.mdp.PolicyIterationModified(P, R, 0.99999, epsilon=0.01, max_iter=10**6, skip_check=True)\n",
    "# forest_pi_mdp.run()\n",
    "# forest_pi_mdp.policy\n",
    "# print(\"forest_pi_mdp.policy\", forest_pi_mdp.policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_simple_data(x_var, y_var, x_label, y_label, title, figure_size=(4,3)):\n",
    "    plt.rcParams[\"figure.figsize\"] = figure_size\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    plt.plot(x_var, y_var, 'o-')\n",
    "    plt.show()\n",
    "\n",
    "def plot_data_legend(x_vars, x_label, all_y_vars, y_var_labels, y_label, title, y_bounds=None):\n",
    "    colors = ['red','orange','black','green','blue','violet']\n",
    "    plt.rcParams[\"figure.figsize\"] = (4,3)\n",
    "\n",
    "    i = 0\n",
    "    for y_var in all_y_vars:\n",
    "#         if i == 2: # don't plot when i = 1 for cv\n",
    "#             x_vars = x_vars[1:]\n",
    "        plt.plot(x_vars, y_var, 'o-', color=colors[i % 6], label=y_var_labels[i])\n",
    "        i += 1\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    if y_bounds != None:\n",
    "        plt.ylim(y_bounds)\n",
    "    leg = plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_time_array(run_stats, variables):\n",
    "    cumulative_sum = 0\n",
    "    times = []\n",
    "    output_dict = {v:[] for v in variables}\n",
    "    output_dict[\"times\"] = times\n",
    "    for result in run_stats:\n",
    "        times.append(result[\"Time\"])\n",
    "        for v in result:\n",
    "            if v in variables:\n",
    "                output_dict[v].append(result[v])\n",
    "    return output_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P, R = hiive.mdptoolbox.example.forest(S=2000, p=0.01)\n",
    "st = time.time()\n",
    "fm_q_mdp = hiive.mdptoolbox.mdp.QLearning(P, R, 0.999, epsilon=0.1,epsilon_decay=0.95, n_iter=1000000, alpha=0.95, skip_check=True)\n",
    "fm_q_mdp.run()\n",
    "end = time.time()\n",
    "end-st\n",
    "fm_q_mdp.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_q_curated_results = make_time_array(fm_q_mdp.run_stats, [\"Mean V\", \"Max V\", \"Iteration\"])\n",
    "num_iters = len(fm_q_curated_results[\"Mean V\"])\n",
    "plot_simple_data(fm_q_curated_results[\"Iteration\"], fm_q_curated_results[\"Mean V\"], \n",
    "                 \"iteration\", \"Mean Value\", \"Q-Learning Forest Mgmt Mean Value over Training\", figure_size=(6,4))\n",
    "plot_simple_data(fm_q_curated_results[\"Iteration\"], fm_q_curated_results[\"Max V\"], \n",
    "                 \"iteration\", \"Max Value\", \"Q-Learning Forest Mgmt Max Value over Training\", figure_size=(6,4))\n",
    "plot_simple_data(fm_q_curated_results[\"Iteration\"], fm_q_curated_results[\"times\"], \n",
    "                 \"iteration\", \"time elapsed (seconds)\", \"Q-Learning Forest Mgmt Time Elapsed over Training\", figure_size=(6,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose_discounts(significant_digits):\n",
    "    prev_discount = 0\n",
    "    discounts = []\n",
    "    for i in range(1,significant_digits + 1):\n",
    "        discounts.append(round(prev_discount + 9*(10**-i),i))\n",
    "        prev_discount = discounts[-1]\n",
    "    return discounts\n",
    "\n",
    "def run_forest(solver, states, discounts, epsilons, probability=0.1, max_iter=10):\n",
    "    experiments = [] #num states, probability, discount, time, iterations, policy\n",
    "    for s in states:\n",
    "        for e in epsilons:\n",
    "            for d in discounts:\n",
    "                entry = {}\n",
    "                P, R = hiive.mdptoolbox.example.forest(S=s, p=probability)\n",
    "                #start_time = time.time()\n",
    "                args = {\"transitions\":P, \"reward\":R, \"gamma\":d, \"epsilon\":e, \"max_iter\":max_iter, \"skip_check\":True}\n",
    "                mdp = solver(args)\n",
    "                mdp.run()\n",
    "                #end_time = time.time()\n",
    "                entry[\"time\"] = mdp.time\n",
    "                entry[\"iterations\"] = mdp.iter\n",
    "                entry[\"policy\"] = mdp.policy\n",
    "                entry[\"num_states\"] = s\n",
    "                entry[\"probability\"] = probability\n",
    "                entry[\"discount\"] = d\n",
    "                entry[\"epsilon\"] = e\n",
    "                entry[\"run_stats\"] = mdp.run_stats\n",
    "                experiments.append(entry)\n",
    "    return experiments\n",
    "       \n",
    "\n",
    "#number of states by time/iterations\n",
    "#discount over time/iterations\n",
    "#p over number of 1's in policy\n",
    "#epsilon over quality\n",
    "\n",
    "#TODO quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [10**s for s in range(1,4)]\n",
    "discounts = compose_discounts(3)\n",
    "discounts = [0.999999,0.9999999]\n",
    "epsilons = [0.01, 0.005, 0.001]\n",
    "\n",
    "fm_policy_iteration = lambda dict_args: hiive.mdptoolbox.mdp.PolicyIterationModified(**dict_args)\n",
    "fm_policy_iteration_results = run_forest(fm_policy_iteration, states, discounts, epsilons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [10**s for s in range(2,4)]\n",
    "discounts = compose_discounts(5)\n",
    "epsilons = [0.01, 0.005, 0.001]\n",
    "\n",
    "\n",
    "fm_value_iteration = lambda dict_args: hiive.mdptoolbox.mdp.ValueIteration(**dict_args)\n",
    "fm_value_iteration_results = run_forest(fm_value_iteration, states, discounts, epsilons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_value_iteration_results\n",
    "def print_training_results(results):\n",
    "    for result in fm_value_iteration_results:\n",
    "        print(\"\\nNew result #################\")\n",
    "        for key in result:\n",
    "            if key != \"policy\":\n",
    "                print(\"{0}: {1}\".format(key,result[key]))\n",
    "def collect_training_results(results, keys, to_print=True):\n",
    "    output_dict = {key:[] for key in keys}\n",
    "    for result in results:\n",
    "        if to_print: print(\"\\nNew result #################\")\n",
    "        for key in result:\n",
    "            if key in keys:\n",
    "                if to_print: print(\"{0}: {1}\".format(key,result[key]))\n",
    "                output_dict[key].append(result[key])\n",
    "    return output_dict\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [k*10**pwr for pwr in range(2,3) for k in range(1,10)]\n",
    "states += [1000]\n",
    "states += [1000 + s for s in (states) ]\n",
    "\n",
    "discounts = [0.9999999]\n",
    "epsilons = [0.1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fm_value_iteration = lambda dict_args: hiive.mdptoolbox.mdp.ValueIteration(**dict_args)\n",
    "fm_value_iteration_results = run_forest(fm_value_iteration, states, discounts, epsilons)  \n",
    "#print_training_results(fm_value_iteration_results)\n",
    "fm_vi_time_num_states = collect_training_results(fm_value_iteration_results, [\"time\", \"num_states\"], to_print=False)\n",
    "fm_vi_iters_num_states = collect_training_results(fm_value_iteration_results, [\"iterations\", \"num_states\"], to_print=False)\n",
    "plot_simple_data(fm_vi_time_num_states[\"num_states\"], fm_vi_time_num_states[\"time\"], \"num_states\", \"time\", \"Forest Mgmt Performance with Value Iteration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_policy_iteration = lambda dict_args: hiive.mdptoolbox.mdp.PolicyIterationModified(**dict_args)\n",
    "fm_policy_iteration_results = run_forest(fm_policy_iteration, states, discounts, epsilons)\n",
    "fm_pi_time_num_states = collect_training_results(fm_policy_iteration_results, [\"time\", \"num_states\"], to_print=False)\n",
    "fm_pi_iters_num_states = collect_training_results(fm_policy_iteration_results, [\"iterations\", \"num_states\"], to_print=False)\n",
    "plot_simple_data(fm_pi_time_num_states[\"num_states\"], fm_pi_time_num_states[\"time\"], \"num_states\", \"time\", \"Forest Mgmt Performance with Policy Iteration\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [2000]\n",
    "discounts = [0.99]\n",
    "epsilons = [0.75,0.5,0.25,0.1,0.01, 0.001]\n",
    "fm_value_iteration = lambda dict_args: hiive.mdptoolbox.mdp.ValueIteration(**dict_args)\n",
    "fm_value_iteration_results = run_forest(fm_value_iteration, states, discounts, epsilons, probability=0.0001, max_iter=10**2)  \n",
    "#print_training_results(fm_value_iteration_results)\n",
    "fm_vi_time_num_states = collect_training_results(fm_value_iteration_results, [\"time\", \"epsilon\", \"iterations\"], to_print=False)\n",
    "plot_simple_data(fm_vi_time_num_states[\"epsilon\"], fm_vi_time_num_states[\"time\"], \"epsilon\", \"time\", \"Forest Mgmt Value Iteration Training Time over Epsilon\")\n",
    "plot_simple_data(fm_vi_time_num_states[\"epsilon\"], fm_vi_time_num_states[\"iterations\"], \"epsilon\", \"iterations\", \"Forest Mgmt Value Iteration Iterations over Epsilon\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [2000]\n",
    "discounts = [0.99]\n",
    "epsilons = [0.75,0.5,0.25,0.1,0.01, 0.001]\n",
    "fm_policy_iteration = lambda dict_args: hiive.mdptoolbox.mdp.PolicyIterationModified(**dict_args)\n",
    "fm_policy_iteration_results = run_forest(fm_policy_iteration, states, discounts, epsilons, probability=0.0001, max_iter=10**2)\n",
    "fm_pi_time_num_states = collect_training_results(fm_policy_iteration_results, [\"time\", \"epsilon\"], to_print=False)\n",
    "fm_pi_iters_num_states = collect_training_results(fm_policy_iteration_results, [\"iterations\", \"epsilon\"], to_print=False)\n",
    "plot_simple_data(fm_pi_time_num_states[\"epsilon\"], fm_pi_time_num_states[\"time\"], \"epsilon\", \"time\", \"Forest Mgmt Performance with Policy Iteration\")\n",
    "plot_simple_data(fm_pi_iters_num_states[\"epsilon\"], fm_pi_iters_num_states[\"iterations\"], \"epsilon\", \"iterations\", \"Forest Mgmt Policy Iteration Iterations over Epsilon\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_pi_fm, R_pi_fm = hiive.mdptoolbox.example.forest(S=2000, p=0.01)\n",
    "dict_args = {\"transitions\":P_pi_fm, \"reward\":R_pi_fm, \"gamma\":0.9999, \"epsilon\":0.1, \"max_iter\":10**3}\n",
    "fm_pi_mdp = hiive.mdptoolbox.mdp.PolicyIteration(P_pi_fm, R_pi_fm, 0.999, max_iter = 5*10**2, skip_check=True)\n",
    "fm_pi_mdp.run()\n",
    "print(fm_pi_mdp)\n",
    "fm_pi_mdp_curated_results = make_time_array(fm_pi_mdp.run_stats, [\"Mean V\", \"Max V\"])\n",
    "num_iters = len(fm_pi_mdp_curated_results[\"Mean V\"])\n",
    "plot_simple_data([i for i in range(num_iters)], fm_pi_mdp_curated_results[\"Mean V\"], \n",
    "                 \"iteration\", \"Mean Value\", \"PI Forest Mgmt Mean Value over Training\", figure_size=(6,4))\n",
    "plot_simple_data([i for i in range(num_iters)], fm_pi_mdp_curated_results[\"Max V\"], \n",
    "                 \"iteration\", \"Max Value\", \"PI Forest Mgmt Max Value over Training\", figure_size=(6,4))\n",
    "plot_simple_data([i for i in range(num_iters)], fm_pi_mdp_curated_results[\"times\"], \n",
    "                 \"iteration\", \"time elapsed (seconds)\", \"PI Forest Mgmt Time Elapsed over Training\", figure_size=(6,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_pi_fm, R_pi_fm = hiive.mdptoolbox.example.forest(S=2000, p=0.01)\n",
    "dict_args = {\"transitions\":P_pi_fm, \"reward\":R_pi_fm, \"gamma\":0.999,\"epsilon\":10**(-50), \"max_iter\":10**5, \"skip_check\":True}\n",
    "fm_vi_mdp = hiive.mdptoolbox.mdp.ValueIteration(**dict_args)\n",
    "fm_vi_mdp.run()\n",
    "print(fm_vi_mdp)\n",
    "fm_vi_mdp_curated_results = make_time_array(fm_vi_mdp.run_stats, [\"Mean V\", \"Max V\"])\n",
    "num_iters = len(fm_vi_mdp_curated_results[\"Mean V\"])\n",
    "print(\"max mean v\", max(fm_vi_mdp_curated_results[\"Mean V\"]))\n",
    "plot_simple_data([i for i in range(num_iters)], fm_vi_mdp_curated_results[\"Mean V\"], \n",
    "                 \"iteration\", \"Mean Value\", \"VI Forest Mgmt Mean Value over Training\", figure_size=(6,4))\n",
    "plot_simple_data([i for i in range(num_iters)], fm_vi_mdp_curated_results[\"Max V\"], \n",
    "                 \"iteration\", \"Max Value\", \"VI Forest Mgmt Max Value over Training\", figure_size=(6,4))\n",
    "plot_simple_data([i for i in range(num_iters)], fm_vi_mdp_curated_results[\"times\"], \n",
    "                 \"iteration\", \"time elapsed (seconds)\", \"VI Forest Mgmt Time Elapsed over Training\", figure_size=(6,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"max mean v pi\", max(fm_pi_mdp_curated_results[\"Mean V\"]))\n",
    "neq = []\n",
    "for i in range(len(fm_vi_mdp.policy)):\n",
    "    if fm_vi_mdp.policy[i] != fm_pi_mdp.policy[i]:\n",
    "        neq.append(i)\n",
    "len(neq)\n",
    "sum(fm_vi_mdp.policy) < sum(fm_pi_mdp.policy)\n",
    "sum(fm_pi_mdp.policy)\n",
    "neq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_pi_fm, R_pi_fm = hiive.mdptoolbox.example.forest(S=2000, p=0.01, r1=1000000)\n",
    "dict_args = {\"transitions\":P_pi_fm, \"reward\":R_pi_fm, \"gamma\":0.999,\"epsilon\":10**(-10), \"max_iter\":10**5, \"skip_check\":True}\n",
    "fm_vi_mdp = hiive.mdptoolbox.mdp.ValueIteration(**dict_args)\n",
    "fm_vi_mdp.run()\n",
    "print(fm_vi_mdp)\n",
    "fm_vi_mdp_curated_results = make_time_array(fm_vi_mdp.run_stats, [\"Mean V\", \"Max V\"])\n",
    "num_iters = len(fm_vi_mdp_curated_results[\"Mean V\"])\n",
    "print(\"max mean v\", max(fm_vi_mdp_curated_results[\"Mean V\"]))\n",
    "plot_simple_data([i for i in range(num_iters)], fm_vi_mdp_curated_results[\"Mean V\"], \n",
    "                 \"iteration\", \"Mean Value\", \"VI Forest Mgmt Mean Value over Training\", figure_size=(6,4))\n",
    "plot_simple_data([i for i in range(num_iters)], fm_vi_mdp_curated_results[\"Max V\"], \n",
    "                 \"iteration\", \"Max Value\", \"VI Forest Mgmt Max Value over Training\", figure_size=(6,4))\n",
    "plot_simple_data([i for i in range(num_iters)], fm_vi_mdp_curated_results[\"times\"], \n",
    "                 \"iteration\", \"time elapsed (seconds)\", \"VI Forest Mgmt Time Elapsed over Training\", figure_size=(6,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"max mean v pi\", max(fm_pi_mdp_curated_results[\"Mean V\"]))\n",
    "neq = []\n",
    "for i in range(len(fm_vi_mdp.policy)):\n",
    "    if fm_vi_mdp.policy[i] != fm_pi_mdp.policy[i]:\n",
    "        neq.append(i)\n",
    "len(neq)\n",
    "sum(fm_vi_mdp.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")\n",
    "env.reset()\n",
    "#Credit Blake Wang CS7641 @709_f1\n",
    "nA, nS = env.nA, env.nS\n",
    "P_fl = np.zeros([nA, nS, nS])\n",
    "R_fl = np.zeros([nS, nA])\n",
    "for s in range(nS):\n",
    "    for a in range(nA):\n",
    "        transitions = env.P[s][a]\n",
    "        for p_trans, next_s, reward, _ in transitions:\n",
    "            P_fl[a,s,next_s] += p_trans\n",
    "            R_fl[s,a] = reward\n",
    "        P_fl[a,s,:] /= np.sum(P_fl[a,s,:])\n",
    "\n",
    "\n",
    "# frozen_q_policy = policy_iteration(frozen_lake_env, gamma = 0.4)\n",
    "# policy_q_score = evaluate_policy(frozen_lake_env, frozen_pi_policy, gamma, n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P, R = hiive.mdptoolbox.example.forest(S=2000, p=0.01)\n",
    "st = time.time()\n",
    "fm_q_mdp = hiive.mdptoolbox.mdp.QLearning(P, R, 0.999, epsilon=0, n_iter=10**7, alpha=0.95, skip_check=True)\n",
    "fm_q_mdp.run()\n",
    "end = time.time()\n",
    "end-st\n",
    "fm_q_mdp.policy\n",
    "fm_q_mdp.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_q_curated_results = make_time_array(fm_q_mdp.run_stats, [\"Mean V\", \"Max V\", \"Iteration\"])\n",
    "plot_simple_data(fm_q_curated_results[\"Iteration\"], fm_q_curated_results[\"Mean V\"], \n",
    "                 \"iteration\", \"Mean Value\", \"Q-Learning Forest Mgmt Mean Value over Training\", figure_size=(6,4))\n",
    "plot_simple_data(fm_q_curated_results[\"Iteration\"], fm_q_curated_results[\"Max V\"], \n",
    "                 \"iteration\", \"Max Value\", \"Q-Learning Forest Mgmt Max Value over Training\", figure_size=(6,4))\n",
    "plot_simple_data(fm_q_curated_results[\"Iteration\"], fm_q_curated_results[\"times\"], \n",
    "                 \"iteration\", \"time elapsed (seconds)\", \"Q-Learning Forest Mgmt Time Elapsed over Training\", figure_size=(6,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_q_mdp = hiive.mdptoolbox.mdp.QLearning(P_fl, R_fl, 0.99, epsilon=0.0,epsilon_decay=.95, n_iter=10**7, alpha=0.95, skip_check=True)\n",
    "fl_q_mdp.run()\n",
    "fm_q_mdp.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_q_curated_results = make_time_array(fl_q_mdp.run_stats, [\"Mean V\", \"Max V\", \"Iteration\"])\n",
    "plot_simple_data(fl_q_curated_results[\"Iteration\"], fl_q_curated_results[\"Mean V\"], \n",
    "                 \"iteration\", \"Mean Value\", \"Q-Learning Frozen Lake Mean Value over Training\", figure_size=(6,4))\n",
    "plot_simple_data(fl_q_curated_results[\"Iteration\"], fl_q_curated_results[\"Max V\"], \n",
    "                 \"iteration\", \"Max Value\", \"Q-Learning Frozen Lake Max Value over Training\", figure_size=(6,4))\n",
    "plot_simple_data(fl_q_curated_results[\"Iteration\"], fl_q_curated_results[\"times\"], \n",
    "                 \"iteration\", \"time elapsed (seconds)\", \"Q-Learning Frozen Lake Time Elapsed over Training\", figure_size=(6,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_args = {\"transitions\":P_fl, \"reward\":R_fl, \"gamma\":0.999,\"epsilon\":10**(-10), \"max_iter\":10**5, \"skip_check\":True}\n",
    "fm_vi_mdp = hiive.mdptoolbox.mdp.ValueIteration(**dict_args)\n",
    "fm_vi_mdp.run()\n",
    "#print(fm_vi_mdp)\n",
    "fm_vi_mdp_curated_results = make_time_array(fm_vi_mdp.run_stats, [\"Mean V\", \"Max V\", \"Iteration\"])\n",
    "num_iters = len(fm_vi_mdp_curated_results[\"Mean V\"])\n",
    "print(\"max mean v\", max(fm_vi_mdp_curated_results[\"Mean V\"]))\n",
    "plot_simple_data(fm_vi_mdp_curated_results[\"Iteration\"], fm_vi_mdp_curated_results[\"Mean V\"], \n",
    "                 \"iteration\", \"Mean Value\", \"VI Forest Mgmt Mean Value over Training\", figure_size=(6,4))\n",
    "plot_simple_data(fm_vi_mdp_curated_results[\"Iteration\"], fm_vi_mdp_curated_results[\"Max V\"], \n",
    "                 \"iteration\", \"Max Value\", \"VI Forest Mgmt Max Value over Training\", figure_size=(6,4))\n",
    "plot_simple_data(fm_vi_mdp_curated_results[\"Iteration\"], fm_vi_mdp_curated_results[\"times\"], \n",
    "                 \"iteration\", \"time elapsed (seconds)\", \"VI Forest Mgmt Time Elapsed over Training\", figure_size=(6,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_args = {\"transitions\":P_fl, \"reward\":R_fl, \"gamma\":0.999,\"epsilon\":10**(-10), \"max_iter\":10**5, \"skip_check\":True}\n",
    "fm_vi_mdp = hiive.mdptoolbox.mdp.ValueIteration(**dict_args)\n",
    "fm_vi_mdp.run()\n",
    "#print(fm_vi_mdp)\n",
    "fm_vi_mdp_curated_results = make_time_array(fm_vi_mdp.run_stats, [\"Mean V\", \"Max V\", \"Iteration\"])\n",
    "num_iters = len(fm_vi_mdp_curated_results[\"Mean V\"])\n",
    "print(\"max mean v\", max(fm_vi_mdp_curated_results[\"Mean V\"]))\n",
    "plot_simple_data(fm_vi_mdp_curated_results[\"Iteration\"], fm_vi_mdp_curated_results[\"Mean V\"], \n",
    "                 \"iteration\", \"Mean Value\", \"VI Forest Mgmt Mean Value over Training\", figure_size=(6,4))\n",
    "plot_simple_data(fm_vi_mdp_curated_results[\"Iteration\"], fm_vi_mdp_curated_results[\"Max V\"], \n",
    "                 \"iteration\", \"Max Value\", \"VI Forest Mgmt Max Value over Training\", figure_size=(6,4))\n",
    "plot_simple_data(fm_vi_mdp_curated_results[\"Iteration\"], fm_vi_mdp_curated_results[\"times\"], \n",
    "                 \"iteration\", \"time elapsed (seconds)\", \"VI Forest Mgmt Time Elapsed over Training\", figure_size=(6,4))\n",
    "\n",
    "\n",
    "# frozen_q_policy = policy_iteration(frozen_lake_env, gamma = 0.4)\n",
    "# policy_q_score = evaluate_policy(frozen_lake_env, frozen_pi_policy, gamma, n=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
